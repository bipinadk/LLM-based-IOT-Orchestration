{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e0eca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "from typing import Dict, List, Optional, Any\n",
    "from fewshots import analyser_fs, planner_fs\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9479c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf51ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeploymentSystem:\n",
    "    def __init__(self):\n",
    "        self.specs: Dict[str, Dict[str, Any]] = {}\n",
    "        self.summary: str = \"\"\n",
    "        self.chat_history: List[str] = []\n",
    "        self.current_prompt: Optional[str] = None\n",
    "        self.plan_history: List[Dict[str, Any]] = []\n",
    "        self.analyzer_fs = analyser_fs\n",
    "        self.planner_fs = planner_fs\n",
    "        # self.optimizer_fs = optimizer_fs\n",
    "        # self.validator_fs = validator_fs\n",
    "        # self.summarizer_fs = summarizer_fs\n",
    "    \n",
    "    def llm(self, prompt: str) -> str:\n",
    "        \"\"\"Call the LLM with the given prompt and return the response.\"\"\"\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            input=prompt\n",
    "        )\n",
    "        return response.output_text\n",
    "    \n",
    "    # Agent 1: Summarizer\n",
    "    def summarizer(self) -> str:\n",
    "        \"\"\"Summarize the current state of the system (devices, plan, etc.).\"\"\"\n",
    "        pass\n",
    "\n",
    "    # Agent 2: Analyzer\n",
    "    def analyzer(self, user_prompt: str) -> str:\n",
    "        \"\"\"Analyze and expand the user's intent from the input prompt and provide a prompt for the planner\"\"\"\n",
    "        self.current_prompt = user_prompt\n",
    "        # Use few-shot examples to guide the analysis\n",
    "        prompt_header = (\"You are an AI system that converts user deployment requests into structured JSON intent to help an LLM planner generate deployment plans. Here are some examples:\\n\\n\")\n",
    "        few_shot_blocks = []\n",
    "        for example in self.analyzer_fs:\n",
    "            formatted=f\"User: {example['prompt']}\\nIntent: {json.dumps(example['output'], indent=2)}\\n\"\n",
    "            few_shot_blocks.append(formatted)\n",
    "        few_shot_text = \"\\n\\n\".join(few_shot_blocks)\n",
    "        full_prompt = f\"{prompt_header}{few_shot_text}\\n\\nUser: {user_prompt}\\nIntent:\"\n",
    "        self.chat_history.append(f\"User:\",user_prompt)\n",
    "        intent = self.llm(full_prompt)\n",
    "        self.chat_history.append(f\"AI:\", intent)\n",
    "        return intent\n",
    "\n",
    "\n",
    "    # Agent 3: Planner\n",
    "    def planner(self, intent: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate a potential plan for device deployment.\"\"\"\n",
    "        prompt_header = (\n",
    "        \"You are an LLM planner that creates deployment plans for IoT–Edge–Cloud systems.\\n\"\n",
    "        \"You will be given:\\n\"\n",
    "        \"1. The current system specifications (devices, edges, clouds)\\n\"\n",
    "        \"2. The user's high-level intent (goals, constraints, preferences)\\n\"\n",
    "        \"3. The last deployment plan (if any)\\n\"\n",
    "        \"4. The original user prompt\\n\"\n",
    "        \"Your job is to provide a JSON plan along with a short 'Chain of Thought' reasoning.\\n\\n\"\n",
    "        \"Here are some examples:\\n\\n\"\n",
    "    )\n",
    "        few_shot_blocks = []\n",
    "        for example in self.planner_fs:\n",
    "            formatted = (\n",
    "                f\"Intent:\\n{json.dumps(example['intent'], indent=2)}\\n\"\n",
    "                f\"Specs:\\n{json.dumps(example['specs'], indent=2)}\\n\"\n",
    "                f\"Last Plan:\\n{json.dumps(example.get('last_plan', {}), indent=2)}\\n\"\n",
    "                f\"Output:\\n{json.dumps(example['output'], indent=2)}\\n\"\n",
    "            )\n",
    "            few_shot_blocks.append(formatted)\n",
    "        few_shot_text = \"\\n\\n\".join(few_shot_blocks)\n",
    "        full_prompt = (\n",
    "            f\"{prompt_header}{few_shot_text}\\n\\n\"\n",
    "            f\"Now generate a new plan.\\n\"\n",
    "            f\"User Prompt: {self.current_prompt}\\n\"\n",
    "            f\"Intent:\\n{intent}\\n\"\n",
    "            f\"Specs:\\n{json.dumps(self.specs, indent=2)}\\n\"\n",
    "            f\"Last Plan:\\n{json.dumps(self.plan_history[-1] if self.plan_history else {}, indent=2)}\\n\"\n",
    "            f\"Output:\"\n",
    "    )\n",
    "        self.chat_history.append((\"Planner:\", self.current_prompt))\n",
    "        plan = self.llm(full_prompt)\n",
    "        self.chat_history.append(f\"AI:\", plan)\n",
    "        return plan # Assuming the plan is a JSON string, i am not checking its validity here\n",
    "    # Agent 4: Optimizer\n",
    "    def optimizer(self, plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Optimize the deployment plan based on latency, bandwidth, etc.\"\"\"\n",
    "        # For simplicity, we will just return the plan as is.\n",
    "        # In a real implementation, this would involve more complex logic.\n",
    "        # Before that, we will make sure the the plan is a dictory object.\n",
    "        if isinstance(plan, str):\n",
    "            try:\n",
    "                plan = json.loads(plan)\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Error decoding plan JSON.\")\n",
    "                return {}\n",
    "        return plan[\"plan\"] if \"plan\" in plan else plan\n",
    "    # Agent 5: Resource Validator\n",
    "    def validate_resources(self, plan: Dict[str, Any]) -> bool:\n",
    "        \"\"\"Check if the plan meets all constraints (CPU, RAM, bandwidth).\"\"\"\n",
    "        pass\n",
    "\n",
    "    # Agent 6: Goal Validator\n",
    "    def validate_goals(self, plan: Dict[str, Any]) -> bool:\n",
    "        \"\"\"Check if the plan meets the user's goals.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Utility: Generate a final plan end-to-end\n",
    "    def generate_plan(self, user_prompt: str):\n",
    "        \"\"\"Run all agents sequentially to create the final plan.\"\"\"\n",
    "        while True:\n",
    "            analysis = self.analyzer(user_prompt)\n",
    "            plan = self.planner(analysis)\n",
    "            optimized_plan = self.optimizer(plan)\n",
    "            self.plan_history.append(optimized_plan)\n",
    "            # Validate resources\n",
    "            while not self.validate_resorces(optimized_plan):\n",
    "                print(\"❌ Resource constraints validation failed.\")\n",
    "                retry = input(\"Would you like to retry the optimization, replan all, or exit? (yes/replan/exit): \").lower()\n",
    "                if retry == 'yes':\n",
    "                    print(\"Retrying optimization...\")\n",
    "                    optimized_plan = self.optimizer(plan)\n",
    "                elif retry == 'replan':\n",
    "                    print(\"Retrying planning from start...\")\n",
    "                    break  # Break inner loop to re-enter outer loop and restart planning\n",
    "                else:\n",
    "                    print(\"Exiting deployment generation.\")\n",
    "                    return\n",
    "            else:\n",
    "                # Only proceed to goal validation if resource validation succeeded\n",
    "                while not self.validate_goals(optimized_plan):\n",
    "                    print(\"❌ Goal validation failed.\")\n",
    "                    retry = input(\"Would you like to retry the optimization, replan all, or exit? (yes/replan/exit): \").lower()\n",
    "                    if retry == 'yes':\n",
    "                        print(\"Retrying optimization...\")\n",
    "                        optimized_plan = self.optimizer(plan)\n",
    "                    elif retry == 'replan':\n",
    "                        print(\"Retrying planning from start...\")\n",
    "                        break  # Break inner loop to re-enter outer loop and restart planning\n",
    "                    else:\n",
    "                        print(\"Exiting deployment generation.\")\n",
    "                        return\n",
    "                else:\n",
    "                    print(\"✅ Plan successfully validated.\")\n",
    "                    return optimized_plan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85551e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "templan = planner_fs[0][\"output\"]\n",
    "DeploymentSystem = DeploymentSystem()\n",
    "plans = DeploymentSystem.optimizer(plan=templan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c53f588a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IoT_Sensors_A': {'type': 'IoT', 'cpu': 0.2, 'ram': 0.1, 'bandwidth_to_edge': 0.5, 'power': 1.5, 'count': 25, 'connected_to': 'Edge_Nodes_A', 'latency': {'Edge_Nodes_A': 5, 'Cloud_Node_1': 50, 'IoT_Sensors_A': 0}}, 'IoT_Sensors_B': {'type': 'IoT', 'cpu': 0.2, 'ram': 0.1, 'bandwidth_to_edge': 0.5, 'power': 1.5, 'count': 25, 'connected_to': 'Edge_Nodes_B', 'latency': {'Edge_Nodes_A': 5, 'Edge_Nodes_B': 5, 'Cloud_Node_1': 50, 'IoT_Sensors_A': 0}}, 'Edge_Node_A': {'type': 'Edge', 'cpu_capacity': 8, 'ram_capacity': 16, 'bandwidth_capacity': 20, 'bandwidth_to_cloud': 5, 'connected_iot': ['IoT_Sensors_A'], 'latency': {'Edge_Node_A': 0, 'Edge_Node_B': 5, 'Cloud_Node_1': 50, 'IoT_Sensor_A': 5, 'IoT_Sensor_B': 5}}, 'Edge_Node_B': {'type': 'Edge', 'cpu_capacity': 8, 'ram_capacity': 16, 'bandwidth_capacity': 20, 'bandwidth_to_cloud': 5, 'connected_iot': ['IoT_Sensors_B'], 'latency': {'Edge_Node_A': 5, 'Edge_Node_B': 0, 'Cloud_Node_1': 50, 'IoT_Sensor_A': 5, 'IoT_Sensor_B': 5}}, 'Cloud_Node': {'type': 'Cloud', 'cpu_capacity': 32, 'ram_capacity': 128, 'bandwidth_capacity': 100, 'latency': {'Edge_Nodes_A': 50, 'Cloud_Node_1': 0, 'IoT_Sensors_A': 50}, 'connected_edge': ['Edge_Nodes_A', 'Edge_Nodes_B']}}\n"
     ]
    }
   ],
   "source": [
    "print(plans)  # Assuming plans is a dictionary, you can print it directly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlbase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
